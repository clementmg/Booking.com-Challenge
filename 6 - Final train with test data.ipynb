{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from statistics import mode\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.pkl\", \"rb\") as f:\n",
    "    test_set, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "model = load_model(\"best_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savez_compressed, load\n",
    "import random\n",
    "def getData(filename1, filename2, filename3, batch_size=512, skip=0.2, train=True):\n",
    "    #initiate a batch count for each size of trips\n",
    "    batch_count = {k: 0 for k in range(50)}\n",
    "    inputs = {k: [] for k in range(50)}\n",
    "    targets = {k: [] for k in range(50)}\n",
    "    while True:\n",
    "        #load files\n",
    "        trip_sequences = load(filename1, allow_pickle=True)[\"arr_0\"]\n",
    "        trip_infos = load(filename2, allow_pickle=True)[\"arr_0\"]\n",
    "        predictions = load(filename3, allow_pickle=True)[\"arr_0\"]\n",
    "        \n",
    "        if train:\n",
    "            random.seed()\n",
    "            #Shuffle\n",
    "            to_shuffle = list(zip(trip_sequences, trip_infos, predictions))\n",
    "            random.shuffle(to_shuffle)\n",
    "            trip_sequences, trip_infos, predictions = zip(*to_shuffle)\n",
    "\n",
    "        #trip_sequences = np.array(trip_sequences)\n",
    "        #trip_infos = np.array(trip_infos) \n",
    "        predictions = np.array(predictions, dtype=\"int32\")\n",
    "        \n",
    "        for i in range(len(trip_sequences)):\n",
    "          \n",
    "            #consider trip i\n",
    "            trip_sequence = trip_sequences[i]\n",
    "            trip_info = trip_infos[i]\n",
    "            prediction = predictions[i]\n",
    "            len_trip = len(trip_sequence)\n",
    "            \n",
    "            #add info to corresponding  size\n",
    "            inputs[len_trip].append([trip_sequence, trip_info])\n",
    "            targets[len_trip].append(prediction)\n",
    "            batch_count[len_trip] += 1\n",
    "            \n",
    "            if batch_count[len_trip] == batch_size:\n",
    "                X_trip = np.array([t[0] for t in inputs[len_trip]], dtype='float32')\n",
    "                X_info = np.array([t[1] for t in inputs[len_trip]], dtype='float32')\n",
    "                y = np.array(targets[len_trip]).reshape(batch_size,)\n",
    "                #yield the data to feed the \n",
    "                yield [X_trip, X_info], y\n",
    "                inputs[len_trip] = []\n",
    "                targets[len_trip] = []\n",
    "                batch_count[len_trip] = 0\n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import sparse_top_k_categorical_accuracy\n",
    "learning_rate = 0.00001\n",
    "batch_size = 512\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=.25)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",        #custom_loss(model, embedding_size, 1, batch_size), # Call the loss function with the model\n",
    "              metrics=['sparse_top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "test_generator = getData(\"X_test_trip.npz\", \"X_test_info.npz\", \"y_test.npz\",\n",
    "                                        batch_size=batch_size, train=True)\n",
    "\n",
    "\n",
    "val_generator = getData(\"X_val_cities.npz\", \"X_val_info.npz\",\n",
    "                          \"y_val.npz\", batch_size=batch_size, train=False)\n",
    "\n",
    "num_train_samples = 10875\n",
    "num_val_samples = 17913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 12s 280ms/step - loss: 12.7224 - sparse_top_k_categorical_accuracy: 0.4614 - val_loss: 12.4191 - val_sparse_top_k_categorical_accuracy: 0.4887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc087d83fa0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model on val generator\n",
    "model.fit(val_generator,\n",
    "          steps_per_epoch=num_val_samples // batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=test_generator,\n",
    "          validation_steps=num_train_samples // batch_size,\n",
    "          batch_size=batch_size,                 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 8s 368ms/step - loss: 12.4310 - sparse_top_k_categorical_accuracy: 0.4784 - val_loss: 12.4966 - val_sparse_top_k_categorical_accuracy: 0.4866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc0680fd9a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model on test_generator\n",
    "\n",
    "model.fit(test_generator,\n",
    "          steps_per_epoch=num_train_samples // batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=val_generator,\n",
    "          validation_steps=num_val_samples // batch_size,\n",
    "          batch_size=batch_size,                 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
